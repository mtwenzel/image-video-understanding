{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 3\n",
    "# Explainable Convolutional Neural Networks\n",
    "\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Imports and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports  { display-mode: \"form\" }\n",
    "#@markdown Import TensorFlow and Tensorflow Probability (TFP). The latter gets you some warnings, but they are not of concern. Also, display GPU information.\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.keras.layers import Input, InputLayer, Conv2D, MaxPool2D, Flatten, Dense, UpSampling2D, LocallyConnected2D, SpatialDropout2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "tfkl = tf.keras.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "# print all devices visible to tensorflow \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "We will this time use a Keras data generator to do data augmentation. It can also take care of splitting the data into training and validation. The test data should be kept independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download and unzip data  { display-mode: \"form\" }\n",
    "#@markdown The data will be downloaded from the Fraunhofer OwnCloud. It is about 150 MB, so it may take a few seconds. Afterwards, the data is unpacked to disk.\n",
    "\n",
    "# importing required modules \n",
    "from zipfile import ZipFile \n",
    "\n",
    "# Download from Fraunhofer OwnCloud \n",
    "!test -e train_val.zip || curl -L \"https://owncloud.fraunhofer.de/index.php/s/x8G8hkM1x4xdf7Z/download\" --output train_val.zip\n",
    "file_name = \"train_val.zip\"\n",
    "with ZipFile(file_name, 'r') as zip: \n",
    "    print('Extracting all the files now...') \n",
    "    zip.extractall() \n",
    "    print('Done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Prepare global variables and training data  { display-mode: \"form\" }\n",
    "#@markdown Depending on the GPU memory, set appropriate image size and batch size. The setting below work for a 4 GB Nvidia GTX1060. Colab allows larger batch sizes.\n",
    "train_data_dir = \"train_val/\" #@param {type:\"string\"}\n",
    "batch_size = 6 #@param {type:\"integer\"}\n",
    "target_width = 512 #@param {type:\"integer\"}\n",
    "target_height = 386 #@param {type:\"integer\"}\n",
    "\n",
    "target_size = (target_width, target_height)\n",
    "\n",
    "#@markdown Set data augmentation parameters\n",
    "shear_range = 0.2 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "zoom_range = 0.2 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "width_shift_range = 0.2 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "height_shift_range = 0.2 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "rotation_range = 10 #@param {type:\"slider\", min:0, max:90, step:5}\n",
    "horizontal_flip = True #@param {type:\"boolean\"}\n",
    "vertical_flip = True #@param {type:\"boolean\"}\n",
    "validation_split = 0.2 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=shear_range,\n",
    "    zoom_range=zoom_range,\n",
    "    width_shift_range=width_shift_range,\n",
    "    height_shift_range=height_shift_range,\n",
    "    rotation_range=rotation_range,\n",
    "    horizontal_flip=horizontal_flip,\n",
    "    vertical_flip=vertical_flip,\n",
    "    validation_split=validation_split) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir, # same directory as training data\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_fn = lambda t: tfd.Normal(loc=t, scale=1)\n",
    "\n",
    "model = tf.keras.Sequential(layers=[\n",
    "    BatchNormalization(input_shape=target_size+(3,)),\n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu', strides=(2,2)),\n",
    "    \n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu', strides=(2,2)),\n",
    "\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=96, kernel_size=(3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=96, kernel_size=(3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=96, kernel_size=(3,3), activation='relu', strides=(2,2)),\n",
    "\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), activation='relu', strides=(2,2)),\n",
    "\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), activation='relu', strides=(2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    BatchNormalization(),\n",
    "    Dense(256),\n",
    "    Dense(2),\n",
    "    tfp.layers.DistributionLambda(d_fn)]\n",
    ")\n",
    "\n",
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=train_generator.samples//batch_size,\n",
    "                              epochs=2,\n",
    "                             validation_data=validation_generator,\n",
    "                             validation_steps=validation_generator.samples//batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Transfer Learning from pretrained networks\n",
    "\n",
    "This code sets up the selected network to have appropriate input sizes.\n",
    "\n",
    "Try to train it, afterwards you can proceed to convert it into a probabilistic network similar to the experiment above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set up a transfer learning model { display-mode: \"form\" }\n",
    "#@markdown Using a Flatten layer creates too many parameters. If you choose to do so, first reduce the number of channels from the output 2048.\n",
    "#@markdown The three available models will in the default setting have quite large numbers of parameters to adjust, still -- ranging from about 2 to about 4 Mio.\n",
    "#@markdown The networks are usually trained on image sizes of about $250^2$, but we apply it to the size set above. \n",
    "\n",
    "\n",
    "apply_flatten = True #@param {type:'boolean'}\n",
    "print_summary = False #@param {type:'boolean'}\n",
    "base_model_name = \"Inception\" #@param [\"Inception\", \"DenseNet121\", 'ResNet50']\n",
    "\n",
    "if base_model_name == 'Inception':\n",
    "    base_model=tf.keras.applications.InceptionV3(weights='imagenet',include_top=False, input_shape=target_size+(3,)) \n",
    "if base_model_name == 'DenseNet121':\n",
    "    base_model=tf.keras.applications.DenseNet121(weights='imagenet',include_top=False, input_shape=target_size+(3,)) \n",
    "if base_model_name == 'ResNet50':\n",
    "    base_model=tf.keras.applications.ResNet50(weights='imagenet',include_top=False, input_shape=target_size+(3,)) \n",
    "\n",
    "x=base_model.output\n",
    "\n",
    "if apply_flatten:\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=128, kernel_size=1, strides=2)(x)\n",
    "    x = Flatten()(x)\n",
    "else:\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x) \n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(512,activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256,activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "preds = Dense(2,activation='softmax')(x) #final layer with softmax activation\n",
    "\n",
    "model = Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "if print_summary:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "trainable_count = int(\n",
    "    np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "non_trainable_count = int(\n",
    "    np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))\n",
    "\n",
    "print('Total params: {:,}'.format(trainable_count + non_trainable_count))\n",
    "print('Trainable params: {:,}'.format(trainable_count))\n",
    "print('Non-trainable params: {:,}'.format(non_trainable_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of this model takes considerable time; approximately 1 minute per epoch. We will not be able to conduct the training in the course, but feel free to run it during a break or as part of your later continuation.\n",
    "\n",
    "Running it for 20 epochs lets one at least observe an increase in training accuracy; on the small dataset, however, it usually overfits (increasing validation loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on the new data for a few epochs just to prove it works in principle.\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=train_generator.samples//batch_size,\n",
    "                              epochs=2,\n",
    "                              validation_data=validation_generator,\n",
    "                              validation_steps=validation_generator.samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training loss')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()\n",
    "print('training acc')\n",
    "plt.plot(history.history['acc'])\n",
    "plt.show()\n",
    "print('validation loss')\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()\n",
    "print('validation acc')\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Segmentation: Auto Encoder (AE)-Style\n",
    "\n",
    "This time, all models will be probabilistic in a certain fashion -- using dropout at inference time, or using probabilistic layers that learn a distribution and sample from it.\n",
    "\n",
    "* We define short functions that return a model.\n",
    "* The first is a simple architecture that collapses and expands an image into the desired mask, similar to an Auto Encoder (AE).\n",
    "* The second is the famous U-Net.\n",
    "\n",
    "Further reading (catchphrases to search in your favourite search engine): \n",
    "* Tensorflow Probability (TFP)\n",
    "* Variational Inference\n",
    "* Bayes by Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download and prepare segementation images and masks {display-mode:\"form\"}\n",
    "\n",
    "!test -e tmp_slices.npz || curl -L \"https://drive.google.com/uc?export=download&id=1R2-H0dhhrj6XNK7Q-MazIWGeFDOf6Zya\" --output tmp_slices.npz\n",
    "\n",
    "TRAINING_SLICE_COUNT = 300 #@param {type:\"slider\", min:100, max:1500, step:100}\n",
    "\n",
    "loaded = np.load('tmp_slices.npz')\n",
    "\n",
    "x_train = loaded['x_train'][:TRAINING_SLICE_COUNT]\n",
    "y_train = loaded['y_train'][:TRAINING_SLICE_COUNT]\n",
    "\n",
    "x_test = loaded['x_train'][TRAINING_SLICE_COUNT:]\n",
    "y_test = loaded['y_train'][TRAINING_SLICE_COUNT:]\n",
    "\n",
    "assert len(x_train) == len(y_train)\n",
    "\n",
    "example_test_slice = 1800 #@param {type:\"integer\"}\n",
    "\n",
    "# remove the lesion labels (values 2..3)\n",
    "y_train_binary = y_train.clip(0, 1)\n",
    "y_test_binary = y_test.clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loaded[\"x_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getDropoutBayesModel(_filters=32, filters_add=0, _kernel_size=(3,3), _padding='same', _activation='relu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid'):\n",
    "    model = Sequential()\n",
    "    # We are indifferent about the xy size, but accept only one channel (gray value images). This has the consequence that debugging sizes gets harder.\n",
    "    input_layer = Input(shape=(None,None,1))\n",
    "    \n",
    "    x = BatchNormalization()(input_layer)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, name='firstConvolutionalLayer')(x)\n",
    "    x = SpatialDropout2D(0.3)(x, training=True)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = SpatialDropout2D(0.3)(x, training=True)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = SpatialDropout2D(0.3)(x, training=True)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = SpatialDropout2D(0.3)(x, training=True)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = Conv2D(1, kernel_size=(1,1), activation=_final_layer_nonlinearity)(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow_probability.python.layers import Convolution2DFlipout\n",
    "\n",
    "def getProbBayesModel(_filters=32, filters_add=0, _kernel_size=(3,3), _padding='same', _activation='relu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid'):\n",
    "    model = Sequential()\n",
    "    # We are indifferent about the xy size, but accept only one channel (gray value images). This has the consequence that debugging sizes gets harder.\n",
    "    input_layer = Input(shape=(None,None,1))\n",
    "    \n",
    "    x = BatchNormalization()(input_layer)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, name='firstConvolutionalLayer')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = Convolution2DFlipout(1, kernel_size=(1,1), activation=_final_layer_nonlinearity)(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow_probability.python.layers import Convolution2DFlipout\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "\n",
    "def getProbBayesModernModel(_filters=32, filters_add=0, _kernel_size=(3,3), _padding='same', _activation='relu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid'):\n",
    "    model = Sequential()\n",
    "    # We are indifferent about the xy size, but accept only one channel (gray value images). This has the consequence that debugging sizes gets harder.\n",
    "    input_layer = Input(shape=(None, None,1))\n",
    "    \n",
    "    x = BatchNormalization()(input_layer)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, strides=(2,2), padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, strides=(2,2), padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2DTranspose(filters=_filters+2*filters_add, kernel_size=_kernel_size, strides=2, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2DTranspose(filters=_filters+filters_add, kernel_size=_kernel_size, strides=2, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = Convolution2DFlipout(1, kernel_size=(1,1), activation=_final_layer_nonlinearity)(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pad_image_for_model(model, input_image):\n",
    "    '''Determine the necessary amount of padding\n",
    "    (difference between input and output size of the model)\n",
    "    and apply it to an ndarry with one or more images.'''\n",
    "    \n",
    "    padding = 0\n",
    "    if 'firstConvolutionalLayer' in [layer.name for layer in model.layers]:\n",
    "        if model.get_layer('firstConvolutionalLayer').padding == 'valid':\n",
    "            padding = 20 # WARNING: Hard-coded for above architecture!\n",
    "\n",
    "            # determine in which dimension to apply this padding\n",
    "            ndim_padding = []\n",
    "            if np.ndim(input_image) > 2:\n",
    "                # do not pad along batch dimension (if present)\n",
    "                ndim_padding.append((0, 0))\n",
    "            ndim_padding.append((padding, padding)) # pad above/below image (y dimension)\n",
    "            ndim_padding.append((padding, padding)) # pad left/right of image (x dimension)\n",
    "            if np.ndim(input_image) > 3:\n",
    "                # do not pad along channel dimension (if present)\n",
    "                ndim_padding.append((0, 0))\n",
    "\n",
    "            input_image = np.lib.pad(input_image, ndim_padding,\n",
    "                                     #'constant', constant_values = 0)\n",
    "                                     'reflect')\n",
    "\n",
    "    return input_image, padding\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class VisualHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # also show initial prediction\n",
    "        plot_prediction(self.model, example_test_slice)\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # show prediction after every training epoch\n",
    "        plot_prediction(self.model, example_test_slice)\n",
    "        \n",
    "vh_callback = VisualHistory()\n",
    "\n",
    "def do_prediction(model, input_image, verbose = False):\n",
    "    # first do padding of full slice\n",
    "    input_image, padding = pad_image_for_model(model, input_image)\n",
    "    \n",
    "    # add batch and channel dimensions (network expects 4D arrays)\n",
    "    input_array = input_image[np.newaxis,:,:,np.newaxis]\n",
    "    if verbose:\n",
    "        print(\"input shape:\", input_array.shape)\n",
    "\n",
    "    y_predicted = model.predict(input_array)\n",
    "    if verbose:\n",
    "        print(\"output shape:\", y_predicted.shape)\n",
    "\n",
    "    return input_image, input_array, y_predicted, padding\n",
    "\n",
    "def plot_prediction(model, pred_slice_index):\n",
    "    # get single slice\n",
    "    input_image    = x_test[pred_slice_index]\n",
    "    # could use y_train_binary here for the first half of the notebook, but in the end we want to see the lesion\n",
    "    reference_mask = y_test[pred_slice_index]\n",
    "\n",
    "    input_image, input_array, y_predicted, padding = do_prediction(model, input_image)\n",
    "    \n",
    "    padded_extent = np.array([0, input_array.shape[2], input_array.shape[1], 0]) - 0.5 - padding\n",
    "\n",
    "    # display prediction for inspection\n",
    "    f, ax = plt.subplots(1, 5 if padding else 4, figsize = (11 if padding else 8, 3), sharey = True)\n",
    "    ax[0].imshow(x_test[pred_slice_index])\n",
    "    ax[0].set_title('orig')\n",
    "    if padding:\n",
    "        ax[1].imshow(input_array[0,:,:,0], extent = padded_extent)\n",
    "        ax[1].set_title('padded input')\n",
    "    ax[-2].imshow(y_predicted[0,:,:,0])\n",
    "    ax[-2].set_title('predicted mask')\n",
    "    ax[-3].imshow(reference_mask.clip(0,1))\n",
    "    ax[-3].set_title('reference mask')\n",
    "    ax[-1].imshow(reference_mask.clip(0,1) - y_predicted[0,:,:,0])\n",
    "    ax[-1].set_title('(ref - predicted)')\n",
    "    ax[0].set_ylim(*padded_extent[2:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "modelBayesValid = getDropoutBayesModel(_padding='valid')\n",
    "modelBayesValid.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(modelBayesValid.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ProbBayes = getProbBayesModel(_padding='valid')\n",
    "model_ProbBayes.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model_ProbBayes.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ProbBayesModern = getProbBayesModernModel(_padding='same')\n",
    "model_ProbBayesModern.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model_ProbBayesModern.count_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for _padding = 'valid' and 'reflect' padding\n",
    "historyBayesValid = modelBayesValid.fit(np.lib.pad(x_train[...,np.newaxis],\n",
    "                                         [(0,0), (20,20), (20,20), (0,0)], 'reflect'),\n",
    "                              y_train_binary[...,np.newaxis],\n",
    "                              batch_size=20, epochs=5, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Conv2D layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for _padding = 'valid' and 'reflect' padding\n",
    "historyProbBayesValid = model_ProbBayes.fit(np.lib.pad(x_train[...,np.newaxis],\n",
    "                                         [(0,0), (20,20), (20,20), (0,0)], 'reflect'),\n",
    "                              y_train_binary[...,np.newaxis],\n",
    "                              batch_size=20, epochs=5, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for _padding = 'same'.\n",
    "# Model needs no padding even for \"valid\"\n",
    "historyProbBayesModernValid = model_ProbBayesModern.fit(x_train[...,np.newaxis],\n",
    "                              y_train_binary[...,np.newaxis],\n",
    "                              batch_size=20, epochs=5, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Bayes by Dropout on the initial model\n",
    "\n",
    "For this approach, the prediction needs to run $n$ times, averaging the results per voxel for the final prediction.\n",
    "\n",
    "Warning: this unconditionally puts all layers into training mode, also the BatchNormalization, which will lead to side effects. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prediction function from the model, setting the learning phase to \"learn\" to let dropout be active.\n",
    "f = K.function([modelValid.layers[0].input, K.learning_phase()],\n",
    "               [modelValid.layers[-1].output])\n",
    "\n",
    "# This takes some memory. In my example, all 566 test images won't be processed on a 4GB GPU.\n",
    "def predict_with_uncertainty(f, x, no_classes, n_iter=20):\n",
    "    result = np.zeros( (n_iter,) + (x.shape) + (no_classes,) )\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        result[i,:, :] = f((x, 1))[0]\n",
    "    prediction = result.mean(axis=0)\n",
    "    uncertainty = result.std(axis=0)\n",
    "    return prediction, uncertainty, result    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on test examples\n",
    "\n",
    "Adapt the following cells that show the principle of prediction, so that\n",
    "* multiple images can be predicted\n",
    "* all model types can be used again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test0 = x_test[0:2][...,np.newaxis]\n",
    "\n",
    "print(x_test0.shape, len(x_test0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test0 = pad_image_for_model(modelValid, x_test0)\n",
    "result = np.zeros((20,) + (len(x_test0),) + (x_test[0].shape) + (1,) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(f((x_test0,1))),f((x_test0,1))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    result[i,:,:,:] = f((x_test0,1))[0]\n",
    "    \n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred0 = do_prediction(modelValid, x_test[0])\n",
    "y_pred1 = do_prediction(modelValid, x_test[1])\n",
    "y_pred0[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = result.mean(axis=0)\n",
    "uncertainty = result.std(axis=0)\n",
    "print(prediction.shape, uncertainty.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(in_image, in_mask, direct_prediction, prob_prediction, uncertainty):\n",
    "    f, ax = plt.subplots(1, 5, figsize = (22, 6), sharey = True)\n",
    "    ax[0].imshow(in_image)\n",
    "    ax[0].set_title('orig')\n",
    "    ax[1].imshow(in_mask)\n",
    "    ax[1].set_title('mask')\n",
    "    ax[2].imshow(direct_prediction)\n",
    "    ax[2].set_title('mask')\n",
    "    ax[3].imshow(prob_prediction)\n",
    "    ax[3].set_title('prob. prediction')\n",
    "    ax[4].imshow(uncertainty)\n",
    "    ax[4].set_title('uncertainty')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(x_test[0], y_test[0], y_pred0[2][0,:,:,0], prediction[0,:,:,0],uncertainty[0,:,:,0])\n",
    "plot_result(x_test[1], y_test[1], y_pred1[2][0,:,:,0], prediction[1,:,:,0],uncertainty[1,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction\n",
    "Let's look at the prediction from some more example slices, but let's only use the `x_test` slices that we did not use for training. (In a real scenario, we would to the separation of training & test data on the level of patients, *before* extracting slices, and we'd also have a validation set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_indices = np.random.choice(x_test.shape[0], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Segmentation using a U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tf.keras.layers import BatchNormalization\n",
    "from tf.keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "\n",
    "def addConvBN(model, filters=32, kernel_size=(3,3), batch_norm=True, activation='prelu', padding='same', kernel_regularizer=None, name = None):\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    if activation == 'prelu':\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name = name))\n",
    "        model.add(PReLU())\n",
    "    elif activation == 'lrelu':\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name = name))\n",
    "        model.add(LeakyReLU())\n",
    "    else:\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation, kernel_regularizer=kernel_regularizer, name = name))\n",
    "\n",
    "from tf.keras import backend as K\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    The jaccard distance loss is useful for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    @url: https://gist.github.com/wassname/7793e2058c5c9dacb5212c0ac0b18a8a\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We will use this to generate the regularisation block for the sequential model.\n",
    "def addConvBNSequential(model, filters=32, kernel_size=(3,3), batch_norm=True, activation='prelu', padding='same', kernel_regularizer=None, name=None):\n",
    "    if batch_norm:\n",
    "        model = BatchNormalization()(model)\n",
    "    if activation == 'prelu':\n",
    "        model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name=name)(model)\n",
    "        model = PReLU()(model)\n",
    "    elif activation == 'lrelu':\n",
    "        model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name=name)(model)\n",
    "        model = LeakyReLU()(model)\n",
    "    else:\n",
    "        model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation, kernel_regularizer=kernel_regularizer, name=name)(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Creates a small U-Net.\n",
    "from tf.keras.layers import Input, concatenate\n",
    "def get_batchnorm_unet(_filters=32, _filters_add=0, _kernel_size=(3,3), _padding='same', _activation='prelu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid', _batch_norm=True):\n",
    "\n",
    "    h, w = x_train.shape[1:]\n",
    "    if _padding == 'valid':\n",
    "        input_layer = Input(shape = (h+40, w+40, 1))\n",
    "    elif _padding == 'same':\n",
    "        input_layer = Input(shape = (h, w, 1))\n",
    "\n",
    "    x0 = addConvBNSequential(input_layer, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm, name = 'firstConvolutionalLayer')\n",
    "    x0 = addConvBNSequential(x0,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x1 = MaxPool2D()(x0)\n",
    "    \n",
    "    x1 = addConvBNSequential(x1,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x1 = addConvBNSequential(x1,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x2 = MaxPool2D()(x1)\n",
    "    \n",
    "    x2 = addConvBNSequential(x2,          filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x2 = addConvBNSequential(x2,          filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x3 = UpSampling2D()(x2)\n",
    "    \n",
    "    x3 = concatenate([x1,x3])\n",
    "    x3 = addConvBNSequential(x3,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x3 = addConvBNSequential(x3,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x4 = UpSampling2D()(x3)\n",
    "    \n",
    "    x4 = concatenate([x0,x4])\n",
    "    x4 = addConvBNSequential(x4,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x4 = addConvBNSequential(x4,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "\n",
    "    output_layer = Conv2D(1, kernel_size=(1,1), activation=_final_layer_nonlinearity)(x4)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tf.keras.utils import plot_model\n",
    "from tf.keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import Image, SVG\n",
    "\n",
    "# (1) plotting to PNG image file\n",
    "plot_model(model, to_file='U-Net.png', show_shapes=False, show_layer_names=True)\n",
    "Image(filename = 'U-Net.png')\n",
    "\n",
    "# (2) plotting to SVG vector graphics format\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_batchnorm_unet(_activation='relu', _batch_norm=True)\n",
    "model.compile(loss=dice_coef_loss, optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))\n",
    "history = model.fit(x_train[...,np.newaxis],\n",
    "                    y_train[...,np.newaxis],\n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "livereveal": {
   "auto_select": "code",
   "auto_select_fragment": "code",
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "default",
   "transition": "linear"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
