{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtwenzel/image-video-understanding/blob/master/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J29FDh2JUNk_"
      },
      "source": [
        "# Self-Attention by Example\n",
        "\n",
        "Partially taken from https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a#8481"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS7dexV7MphR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AoHRZd3SdJ0"
      },
      "source": [
        "## Create a tensor with input data\n",
        "\n",
        "In a real setting, this tensor would be the result of some data encoding step (when considering the first input to the first attention layer), or the result of the previous attention layer.\n",
        "\n",
        "If for example your input is text, and your word embeddings have 512 dimensions, each row in the tensor $x$ would have 512 entries, and the tensor would have as many rows as your sentence has words.\n",
        "\n",
        "For images (here described along the lines of the ViT), the number of rows in $x$ is the number of tiles the images get subdivided into ($16 \\times 16$ in ViT). The length of each line is the length of the embedding vector after transforming each tile with the patch encoder. [See the ViT publication for details](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "Note that there is the embedding size of the original data, but also the internal encoding size that will be determined by the shape of the K, Q, and V matrices."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define data matrix {run:\"auto\"}\n",
        "#@markdown We create random(!) data of a given size. Set the number of tokens and the encoding/embedding length here.\n",
        "num_tokens = 6 #@param {type:\"slider\", min:\"1\", max:\"16\"}\n",
        "num_embedding_features = 9 #@param {type:\"slider\", min:\"1\", max:\"32\"}\n",
        "\n",
        "#@markdown Note that this enables us to show the process in the following, but that this is not describing a real task.\n",
        "\n",
        "x = torch.rand([num_tokens,num_embedding_features])\n",
        "x"
      ],
      "metadata": {
        "id": "h_FPHgn8fTEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-E_fY_cdNJwO"
      },
      "outputs": [],
      "source": [
        "#@title Define second data matrix for cross attention {run:\"auto\"}\n",
        "#@markdown To demonstrate cross attention with smaller attention matrix analogous to Perceiver or DETR, create a \"learned queries\" matrix $x_2$\n",
        "#@markdown For consistency, you can only adjust the number of tokens. The embedding dimension is kept from above.\n",
        "num_ca_tokens = 3 #@param {type:\"slider\", min:\"1\", max:\"10\"}\n",
        "\n",
        "x2 = torch.rand([num_ca_tokens, num_embedding_features])\n",
        "x2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmV9pihNSumD"
      },
      "source": [
        "## Create a set of weight tensors. \n",
        "We are looking at single-head attention only for the moment. For multi-head attention, each weight matrix would be replicated (with independent weights) for each head. You will see this in the second half of the notebook.\n",
        "\n",
        "Each weight tensor has to have as many rows as the tokens have dimensions. Our input vectors have ```num_embedding_features``` dimensions. Let's create random weight matrices of the according size. You are free to select the other dimension, which will then be the internal embedding dimension.\n",
        "\n",
        "Observe how the size of these matrices does not depend on the number of tokens anymore.\n",
        "\n",
        "Note that this will result in a matrix output after the attention mechanism, instead of a single token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSd8O7IANOK5"
      },
      "outputs": [],
      "source": [
        "#@title Get K, Q, V transform matrices {run:\"auto\"}\n",
        "\n",
        "internal_embedding_dimensions = 7 #@param {type:\"slider\", min:\"1\", max:\"32\"}\n",
        "\n",
        "w_key = torch.rand([num_embedding_features,internal_embedding_dimensions])\n",
        "w_query = torch.rand([num_embedding_features,internal_embedding_dimensions])\n",
        "w_value = torch.rand([num_embedding_features,internal_embedding_dimensions])\n",
        "\n",
        "print(f'Initialized random tensors w_key {tuple(w_key.shape)}, w_query {tuple(w_query.shape)}, w_value {tuple(w_value.shape)}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqLOpyeETMBN"
      },
      "source": [
        "## K, Q, and V\n",
        "\n",
        "The actual keys, querys and values are the result of the multiplication of input tensor with weight tensors.\n",
        "\n",
        "Their dimension is:\n",
        "* each row has as many entries as the weight tensors (three in our setup)\n",
        "* the number of rows equals the number of input tokens (five in our setup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXXZUtWWNSjM"
      },
      "outputs": [],
      "source": [
        "# The '@' operator performs matrix multiplication in python,\n",
        "# (in pytorch that is equivalent to `torch.matmul()`)\n",
        "keys = x @ w_key\n",
        "querys = x @ w_query\n",
        "values = x @ w_value\n",
        "\n",
        "# This would be the cross attention with a potentially different number of tokens\n",
        "xattn_q = x2 @ w_query\n",
        "\n",
        "\n",
        "print(\"Keys:\",keys)\n",
        "print(\"Queries:\",querys)\n",
        "print(\"Cross-attention Queries:\",xattn_q)\n",
        "print(\"Values:\",values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNK5GEeBTfKC"
      },
      "source": [
        "## Softmax Attention\n",
        "\n",
        "The size of the square attention matrix equals the number of input tokens in both dimensions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoT9rEVZNVIN"
      },
      "outputs": [],
      "source": [
        "# keys.T transposes the keys matrix\n",
        "attn_scores = querys @ keys.T\n",
        "attn_scores_softmax = softmax(attn_scores, dim=-1)\n",
        "\n",
        "# For readability, round the scores to a definable number of decimal places\n",
        "print(attn_scores_softmax.round(decimals = 2))\n",
        "\n",
        "# Plot self attention matrix\n",
        "sns.heatmap(attn_scores_softmax.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do the same for \"cross attention\""
      ],
      "metadata": {
        "id": "purX4lQ8XkJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xattn_scores = xattn_q @ keys.T\n",
        "xattn_scores_softmax = softmax(xattn_scores, dim=-1)\n",
        "\n",
        "# For readability, round the scores to a definable number of decimal places\n",
        "print(xattn_scores_softmax.round(decimals = 2))\n",
        "\n",
        "# Plot self attention matrix\n",
        "sns.heatmap(xattn_scores_softmax.numpy(), square = True)"
      ],
      "metadata": {
        "id": "hM8FSWsWXc_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUABFsoQTyAV"
      },
      "source": [
        "## Multiply softmax attention with V to obtain the result\n",
        "\n",
        "In a transformer, dense layers would follow that can \n",
        "* reduce a multi-head attention result\n",
        "* enforce correct dimensionality to use output in next input.\n",
        "\n",
        "We will see this after the following experiment with multi-head attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKW44_0GNjAU"
      },
      "outputs": [],
      "source": [
        "weighted_values = values[:,None] * attn_scores_softmax.T[:,:,None]\n",
        "outputs = weighted_values.sum(dim=0)\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention"
      ],
      "metadata": {
        "id": "_nVYTdZwwLvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define number of heads, and create according random weight matrices K, Q, V {run:\"auto\"}\n",
        "#@markdown The only required change is to stack multiple K, Q, V. \n",
        "\n",
        "num_heads = 4 #@param {type:\"slider\", min:\"2\", max:\"10\"}\n",
        "\n",
        "w_key = torch.rand([num_heads, num_embedding_features,internal_embedding_dimensions])\n",
        "w_query = torch.rand([num_heads,num_embedding_features,internal_embedding_dimensions])\n",
        "w_value = torch.rand([num_heads,num_embedding_features,internal_embedding_dimensions])\n",
        "print(f'Initialized new random tensors w_key {tuple(w_key.shape)}, w_query {tuple(w_query.shape)}, w_value {tuple(w_value.shape)}.')"
      ],
      "metadata": {
        "id": "uKbJPwXkOoPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNyAD9WWZYfh"
      },
      "outputs": [],
      "source": [
        "keys = x @ w_key\n",
        "querys = x @ w_query\n",
        "\n",
        "xattn_q = x2 @ w_query\n",
        "\n",
        "values = x @ w_value\n",
        "\n",
        "print(keys)\n",
        "#print(querys)\n",
        "#print(xattn_q)\n",
        "#print(values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# .mT transposes the last two dimensions\n",
        "# (transposing each keys matrix, independently for each head)\n",
        "attn_scores = querys @ keys.mT \n",
        "attn_scores_softmax = softmax(attn_scores, dim=-1)\n",
        "\n",
        "def show_multihead_attention(attn_scores_softmax, column_count = 2):\n",
        "  '''Plot self attention matrices for different heads'''\n",
        "  row_count = num_heads // column_count\n",
        "  f, axx = plt.subplots(row_count, column_count,\n",
        "                        sharex = True, sharey = True,\n",
        "                        figsize = (column_count * 2, row_count * 2))\n",
        "  for ax, t in zip(axx.ravel(), attn_scores_softmax.numpy()):\n",
        "    sns.heatmap(t, ax = ax, square = True)\n",
        "\n",
        "show_multihead_attention(attn_scores_softmax)"
      ],
      "metadata": {
        "id": "bKW_rBOtZe4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Again, do the same for \"cross attention\""
      ],
      "metadata": {
        "id": "e-Te_yTmfhWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xattn_scores = xattn_q @ keys.mT\n",
        "xattn_scores_softmax = softmax(xattn_scores, dim=-1)\n",
        "\n",
        "show_multihead_attention(xattn_scores_softmax)"
      ],
      "metadata": {
        "id": "IE_QyISRZh_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_values = values[:,:,None] * attn_scores_softmax.mT[:,:,:,None]\n",
        "outputs = weighted_values.sum(dim=1)\n",
        "print(outputs)\n",
        "outputs.shape"
      ],
      "metadata": {
        "id": "bJH6eJC7Zkoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert into expected shape for next layer\n",
        "\n",
        "A MLP is employed to \"fix the dimensions\". We require the output shape to match the original $x$ input shape. This can be achieved by creating a MLP weight matrix of the appropriate shape. \n",
        "\n",
        "It has one dimension given by the number of tokens times the internal embedding dimension, the other by the number of original embedding features.\n",
        "\n",
        "Notice that the MLP will require a 2D tensor input -- therefore the heads' outputs need to be flattened. The MLP can then mix the results from the different heads (per token).\n",
        "\n",
        "Consequentially, the MLP will have a rather large weight matrix."
      ],
      "metadata": {
        "id": "-L_UzVeSampm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create MLP weights\n",
        "The size of the MLP output needs to match the original $x$ input data matrix.\n",
        "This makes it possible to stack Attention blocks. Therefore, it has no free parameters."
      ],
      "metadata": {
        "id": "idGI0UqrhNmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_heads = outputs.permute(1,0,2).flatten(1) # The heads are combined. The MLP will mix their results.\n",
        "\n",
        "mlp_weights = torch.rand([num_heads*internal_embedding_dimensions, num_embedding_features])\n",
        "\n",
        "print(\"MLP size:\", mlp_weights.shape)\n",
        "result = combined_heads @ mlp_weights\n",
        "print(\"Resulting shape: \", result.shape)\n",
        "print(\"Resulting new input to next attention block:\", result)\n",
        "\n",
        "assert result.shape==x.shape"
      ],
      "metadata": {
        "id": "4wAF_IZsaZTS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "cd15cf184ab1178a055b183ecb11f28577274e2cc0c29065c013381fe9f37159"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}